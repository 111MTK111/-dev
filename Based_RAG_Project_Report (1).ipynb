{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "--------------Proje Raporu--------------\n",
        "\n",
        "Öncelikle bu projede kaynak görevi gören log dosyasını aşağıdaki kodlar vasıtası ile oluşturdum.\n",
        "Bu şekilde yapmamın nedeni bana verilen ödevde verileri kendimiz oluşturmamız gerektiği idi.\n"
      ],
      "metadata": {
        "id": "08gptfsmyz7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "# Function to generate random IP addresses\n",
        "def generate_ip():\n",
        "    return \".\".join(str(random.randint(0, 255)) for _ in range(4))\n",
        "\n",
        "# Function to generate random timestamps within a specific date range\n",
        "def generate_timestamp(start, end):\n",
        "    return start + (end - start) * random.random()\n",
        "\n",
        "# Function to generate random HTTP methods\n",
        "def generate_http_method():\n",
        "    return random.choice([\"GET\", \"POST\", \"DELETE\", \"PUT\"])\n",
        "\n",
        "# Function to generate random URL paths\n",
        "def generate_url_path():\n",
        "    categories = [\"shirts\", \"pants\", \"jackets\", \"shoes\", \"accessories\"]\n",
        "    items = {\n",
        "        \"shirts\": [\"tshirt\", \"poloshirt\", \"dressshirt\"],\n",
        "        \"pants\": [\"jeans\", \"shorts\", \"trousers\", \"bootcut\"],\n",
        "        \"jackets\": [\"leather\", \"denim\", \"parka\"],\n",
        "        \"shoes\": [\"sneakers\", \"boots\", \"sandals\"],\n",
        "        \"accessories\": [\"hats\", \"scarves\", \"belts\"]\n",
        "    }\n",
        "    category = random.choice(categories)\n",
        "    item = random.choice(items[category])\n",
        "    page = random.choice([\"details\", \"reviews\", \"photos\"])\n",
        "    return f\"/{category}/{item}/{page}\"\n",
        "\n",
        "# Function to generate random HTTP status codes\n",
        "def generate_status_code():\n",
        "    return random.choice([200, 301, 404, 500])\n",
        "\n",
        "# Function to generate a log line\n",
        "def generate_log_line():\n",
        "    ip = generate_ip()\n",
        "    timestamp = generate_timestamp(datetime.datetime(2023, 1, 1), datetime.datetime(2023, 12, 31)).strftime('%d/%b/%Y:%H:%M:%S')\n",
        "    method = generate_http_method()\n",
        "    url_path = generate_url_path()\n",
        "    protocol = \"HTTP/1.1\"\n",
        "    status_code = generate_status_code()\n",
        "    return f'{ip} - - [{timestamp}] \"{method} {url_path} {protocol}\" {status_code} -'\n",
        "\n",
        "# Generate 2000 log lines\n",
        "log_lines = [generate_log_line() for _ in range(2000)]\n",
        "\n",
        "# Create a DataFrame from log lines\n",
        "df_logs = pd.DataFrame(log_lines, columns=[\"log\"])\n",
        "\n",
        "# Save to a CSV file\n",
        "log_file_path = \"/mnt/data/www_111mtk111_com_log.csv\"\n",
        "df_logs.to_csv(log_file_path, index=False)\n",
        "\n",
        "# Show a portion of the dataframe to the user\n",
        "df_logs.head()\n"
      ],
      "metadata": {
        "id": "6vRz9KKZywAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sonrasında elde ettiğim csv dosyası üzerinde düzenlemeler yaparak veriyi eğitime hazır\n",
        "ve üzerinde çalışılabilir hale getirdim.\n",
        "Başta tek bir log sütunu vardı ve veriler iç içe girmiş vaziyette idi. Regex ile sütunları parçalara ayırarak dataframe'i düzenledim.\n",
        "Ardından timestamp sütunundaki ifadeleri datetime formatına çevirmek gerekiyordu. Bu işlem yapıldı.\n"
      ],
      "metadata": {
        "id": "atjM_hUOzLf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_pattern = re.compile(r'(\\S+) (\\S+) (\\S+) \\[(.*?)\\] \"(.*?)\" (\\d{3}) (\\S+)')\n",
        "\n",
        "\n",
        "df[['ip', 'identity', 'user', 'timestamp', 'request', 'status_code', 'size']] = df['log'].str.extract(log_pattern)\n",
        "df.drop(columns=['log', 'size'], inplace=True)\n",
        "\n",
        "\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'], format='%d/%b/%Y:%H:%M:%S')"
      ],
      "metadata": {
        "id": "yjRCnuqB0oPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ardından vektörleştirme işlemi için tüm sütunları birleştirip bir bağlam oluşturdum."
      ],
      "metadata": {
        "id": "2lyvyHf31CDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['combined'] = df['request'] + \" | Status Code: \" + df['status_code'] + \" | IP: \" + df['ip'] + \" | Timestamp: \" + df['timestamp'].astype(str)\n",
        "\n",
        "combined_embeddings = model.encode(df['combined'].tolist())"
      ],
      "metadata": {
        "id": "1hoRutHA1u2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sonrasında Sentence-BERT modelini yükleyerek vektörleştirmeye hazır hale getirdim ve ardından bu işlemi yaptım. Koddaki model dışında birkaç model daha denedim ancak son olarak bu modelde karar kıldım. Proje için daha uygun olduğu kanısındaydım."
      ],
      "metadata": {
        "id": "LmdAGYWd2hom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "combined_embeddings = model.encode(df['combined'].tolist())"
      ],
      "metadata": {
        "id": "qSLetjWy14jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Projenin Retrieval kısmı için bir FAISS index oluşturdum ve ardından vektörleri bu indexe ekledim.\n"
      ],
      "metadata": {
        "id": "dkLWDpEx3Km6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = faiss.IndexFlatL2(combined_embeddings.shape[1])\n",
        "\n",
        "index.add(np.array(combined_embeddings))"
      ],
      "metadata": {
        "id": "G6b1Xr1k3DVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kulanıcıdan doğal dilden soruları alabilmek için bir panel gerekli olduğu için basit bir panel oluşturdum.\n"
      ],
      "metadata": {
        "id": "3WpysycB4_RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = input(\"Sormak istediğiniz soruyu girin: \")"
      ],
      "metadata": {
        "id": "LZRtvxLr5TTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kullanıcıdan alınan soruları vektörleştirme ve ardından FAISS indexinde en yakın sonuçlara ulaşulması ve bulunan log kayıtlarını DF'den çekme işlemleri yapıldı."
      ],
      "metadata": {
        "id": "ixmezEdZ5X3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_embedding = model.encode([user_query])\n",
        "\n",
        "D, I = index.search(query_embedding, k=5)\n",
        "\n",
        "similar_logs = df.iloc[I[0]]\n",
        "print(similar_logs)"
      ],
      "metadata": {
        "id": "1973EFrk5Wun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu kısım daha önce tasarladığım sistemin sağlıklı sonuç vermemesi üzere eklediğim bir kısım. Soru- Cevap çiftleri halinde bulunan eğitim verileri oluşturularak modelin daha iyi eğitilmesi ve sistemin daha sağlıklı sonuç vermesi sağlanabilir. Bu kısımda sadece gösterme amaçlı yer verdim. Yeterli zamanım kalmamıştı."
      ],
      "metadata": {
        "id": "RmO6ksAB6N-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = [\n",
        "    {\"question\": \"Kaç adet status_code değeri 200’dür?\", \"answer\": \"200 adet status_code bulunmaktadır.\"},\n",
        "    {\"question\": \"En çok erişilen sayfa nedir?\", \"answer\": \"GET /index.html\"},\n",
        "    # Diğer soru-cevap çiftleri buraya eklenebilir.\n",
        "]\n",
        "\n",
        "train_df = pd.DataFrame(training_data)"
      ],
      "metadata": {
        "id": "Hryx_QBI7O1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sonrasında RAG modelinin jeneratif model kısmını kurmak için T5 seçildi. GPT seçmememin nedeni ise API'nın kullanım sıkıntısı oldu.\n",
        "\n",
        "FAISS'den alınan verilerin kullanıcıya doğal dilde verilmesi gerektiği için metin verisini tokenlara yani sayısal dizilere dönüştürmek gerekti."
      ],
      "metadata": {
        "id": "Sg2IZPAK7vhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"t5-small\"\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "ZhObX4_08wjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gösterme amaçlı hazırladığım eğitim verilerinin T5 modelinin anlaması için aşağıdaki kod dizimini kullandım."
      ],
      "metadata": {
        "id": "ToZcA0JI82gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "    input_texts = []\n",
        "    target_texts = []\n",
        "    for item in data:\n",
        "        input_text = f\"question: {item['question']} context: Log verileri\"\n",
        "        target_text = item['answer']\n",
        "        input_texts.append(input_text)\n",
        "        target_texts.append(target_text)\n",
        "    return input_texts, target_texts\n",
        "\n",
        "input_texts, target_texts = preprocess_data(training_data)"
      ],
      "metadata": {
        "id": "q9c7F5qR9dAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "İnput ve Output metinlerini tokenize etme işlemi için gerekli kodlar yazıldı."
      ],
      "metadata": {
        "id": "34zZg3oR-U6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = t5_tokenizer(input_texts, truncation=True, padding=True, max_length=512)\n",
        "train_labels = t5_tokenizer(target_texts, truncation=True, padding=True, max_length=512)\n"
      ],
      "metadata": {
        "id": "1h93yu97-eel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aşağıdaki kod eğitim verisinin modelin anlayabileceği bir veri setine dönüştürür."
      ],
      "metadata": {
        "id": "Csejiv9K-kLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = {\n",
        "    \"input_ids\": train_encodings[\"input_ids\"],\n",
        "    \"attention_mask\": train_encodings[\"attention_mask\"],\n",
        "    \"labels\": train_labels[\"input_ids\"]\n",
        "}\n"
      ],
      "metadata": {
        "id": "XL1DqoLL_g81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sistemi tasarlarken istediğim cevapları alamamam sonucunda daha özel bir eğitim yapmak amacı ile fine-tunning işlemini kullanmaya karar verdim. Gerekli kodları oluşturarak eğitim parametleri belirlenmiş oldu."
      ],
      "metadata": {
        "id": "hIIKsUG0_0xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        ")"
      ],
      "metadata": {
        "id": "RzqwhJyKAXFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verilerin organize edilmesi ve modelin etkin bir şekilde eğitilmesi için gerekli kodlar yazıldı."
      ],
      "metadata": {
        "id": "BNQ46ovJAZWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "rwQNxs0xBAeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelin fine-tunning sürecini başlatmak için gerekli olan veri setini ve eğitim yapılandırması sağlanması amacı ile gerekli kodlar oluşturuldu ve model fine-tune edildi."
      ],
      "metadata": {
        "id": "IwId24U7BoHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = QADataset(train_encodings, train_labels[\"input_ids\"])\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=t5_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=t5_tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "p6vT8o5jB015"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FAISS aramsı sonucunda elde ettiğmiz en yakın log kayıtlarını birleştirip metin formatında bir bağlam oluşturmamız gerekiyor idi. FAISS'ten dönen log kayıtları bağlama çevirildi ve sonrasında zengin bir bağlam oluşturuldu."
      ],
      "metadata": {
        "id": "gy8urKSICZ7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_text = \" \".join(similar_logs['combined'].tolist())\n",
        "\n",
        "input_text = f\"question: {user_query} context: {log_text}\""
      ],
      "metadata": {
        "id": "lYLdizNzCkUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize etme işlemi uygulanarak modelin işleyebileceği sayısal verilere dönüştürüldü."
      ],
      "metadata": {
        "id": "CFc3uA3PDab_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = t5_tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)"
      ],
      "metadata": {
        "id": "NE58UqWdDlRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelin soru ve bağlamdan bir yanıt üretmesi sağlandı."
      ],
      "metadata": {
        "id": "HrVlfRsQD6wJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = t5_model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)"
      ],
      "metadata": {
        "id": "fUNpWeK0EA8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Son olarak model tarafından oluşturulan sayısal çıktıyı yani tokenleri tekrar metin formatına çevirdim."
      ],
      "metadata": {
        "id": "otipokjlEJvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"Modelin cevabı: {answer}\")"
      ],
      "metadata": {
        "id": "zAOCH0EREVB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model değerlendirmesi:"
      ],
      "metadata": {
        "id": "hKmKQA60EeXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model çok basit veya kolay sorulara cevap verebiliyor durumda olduğundan daha fazla geliştirilmesi gerekiyor. Özellikle RAG kısmındaki bilgi yetersizliğimden kaynaklı sorunlar yaşadım ancak biraz daha zaman ayırmam durumunda projeyi çok daha iyi yerlere götürebilirim.\n",
        "\n",
        "Log dosyası gerçek verileri içermesi halinde çıkaracağım sonuçlar çok daha sağlıklı olacak ve anlamlı bir analiz yaparak modeli daha iyi eğitme fırsatım olacaktı. O yüzden gerçek bir log dosyasına erişmek de büyük önem taşıyor.\n",
        "\n",
        "Kendi performansımı değerlendirecek olursam:\n",
        "\n",
        "Elimden geleni yaptığımı düşünüyorum kesinlikle. Proje sağlıklı olarak çalışmasa da bir hayli yol kat etmiş durumda ve biraz daha süre verilmesi halinde kusursuza yakın bir sistem tasarlayabileceğim kanısındayım.\n",
        "\n",
        "Son olarak ödevin yapım aşamasında çok fazla şey öğrendim ve bundan dolayı çok memnumum. Teşekkür ederim."
      ],
      "metadata": {
        "id": "4i2ZB_zuEn33"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1-bOS9t4Ej6u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}